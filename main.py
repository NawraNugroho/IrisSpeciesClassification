# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W-bgYhFdam6YWrd5brAiusm1f07hbHKk
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn import datasets

# load iris dataset from scikit-learn
iris = datasets.load_iris()

X = iris.data    # for machine learning input
y = iris.target  # the output
species = iris.target_names

# convert data feature and the target output into DataFrame
df_X = pd.DataFrame(X, columns=iris.feature_names)
df_y = pd.Series(y, name='species')

df_X

df_y

# combine data feature and target into one single DataFrame
df = pd.concat([df_X, df_y], axis=1)

df.head(10)

df.info()

df.describe()

df.corr()

from sklearn.model_selection import train_test_split

# Splitting data for training and testing
# x_train are inputted data for training, y_train is the result data from training
# x_test are inputted data for testing, y_tes is the result data from testing

X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=37)

# training the KNN (K-Nearest Neighbour) model

k = 4  # Number of neighbors
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Using KNN model")
print(f"Accuracy: {accuracy * 100:.2f}%")

print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

accuracy = []
k_values = range(1, 21)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    accuracy.append(accuracy_score(y_test, y_pred))

# Plot accuracy vs. k
plt.figure(figsize=(8,5))
plt.plot(k_values, accuracy, marker='o', linestyle='dashed', color='blue')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('KNN Accuracy for Different k Values')
plt.show()

dtree = DecisionTreeClassifier(criterion='gini',
                               max_depth=3,
                               random_state=37)
dtree.fit(X_train, y_train)

y_pred = dtree.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)

print("Using Decision Tree model")
print(f"Accuracy: {accuracy * 100:.2f}%" )

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# Plot Confusion Matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel("Prediction")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=iris.target_names))

plt.figure(figsize=(12, 8))
plot_tree(dtree, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.show()

accuracy = []
depths = range(1, 11)

for d in depths:
    dtree = DecisionTreeClassifier(max_depth=d, random_state=42)
    dtree.fit(X_train, y_train)
    y_pred = dtree.predict(X_test)
    accuracy.append(accuracy_score(y_test, y_pred))

# Plot accuracy vs. max_depth
plt.figure(figsize=(8, 5))
plt.plot(depths, accuracy, marker='o', linestyle='dashed', color='blue')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Decision Tree Accuracy for Different Depths')
plt.show()